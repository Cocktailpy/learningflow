{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\develop\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks \n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = make_imbalance(iris.data, iris.target, ratio={0: 25, 1: 50, 2: 50},\n",
    "                      random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_missing_value(data, col):\n",
    "    a_mean = data[col].replace(0, np.NaN).mean()\n",
    "    a_std = data[col].replace(0, np.NaN).std()\n",
    "    null_count = data[col].value_counts()[0]\n",
    "    min_val = a_mean - a_std\n",
    "    median =data[col].replace(0, np.NaN).median()\n",
    "    print(col, a_mean, a_std, min_val, median)\n",
    "    a_null_random_list = [median] * null_count  # np.random.randint(min_val if min_val > 0 else 0, a_mean + a_std, size=null_count)\n",
    "    data[col][data[col] == 0] = a_null_random_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对象字段\n",
    "category_columns = []\n",
    "# 数值字段\n",
    "numeric_columns = []\n",
    "\n",
    "encoders = {\n",
    "    'categorical': {},\n",
    "    'scaler': None,\n",
    "    'paid': None\n",
    "}\n",
    "\n",
    "def get_data_sets(df=None,stime=None, etime=None, train = True, remove=False):\n",
    "    if df is None:\n",
    "        df = db_get(stime, etime)\n",
    "    df = df.fillna(0)\n",
    "    df['age'][df['age'] <= 20] = 1\n",
    "    df['age'][(df['age'] > 20) & (df['age'] <=30)] = 2\n",
    "    df['age'][(df['age'] > 30) & (df['age'] <=40)] = 3\n",
    "    df['age'][df['age'] > 40] = 4\n",
    "    # 成交金额分类\n",
    "    df['paid'] = np.cast['int64'](np.ceil(df['src_price'] > 0))\n",
    "    \n",
    "    if train:\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype != 'object' and col != 'src_price' and col != 'res_id' and col != 'paid' and col != 'buy':\n",
    "                numeric_columns.append(col)\n",
    "        \n",
    "        encoders['scaler'] = preprocessing.MinMaxScaler()\n",
    "        encoders['scaler'].fit(df[numeric_columns])\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col not in ['src_price', 'res_id', 'paid', 'buy']:\n",
    "                category_columns.append(col)\n",
    "            \n",
    "            if df[col].dtype == 'object':\n",
    "                cc = pd.Categorical(df[col], ordered=True)\n",
    "                encoders['categorical'][col] = cc.categories\n",
    "                \n",
    "    for c in encoders['categorical']:\n",
    "        e = encoders['categorical'][c]\n",
    "        cc = pd.Categorical(df[c], encoders['categorical'].get(c))\n",
    "        df[c]  = cc.codes   \n",
    "    if remove:\n",
    "        df = df[(df.shape[1] - df.apply(lambda x: sum(x.values == 0), axis=1)) > 10]\n",
    "    missing_cols = ['input_type', 'need_name', 'job', 'phone_home',\n",
    "       'education', 'marriage', 'income', 'emotion_time']\n",
    "    for col in missing_cols:\n",
    "        df = fix_missing_value(df, col)\n",
    "    feature = np.cast['float32'](df[category_columns].values)\n",
    "    labels = df['paid'].values\n",
    "    return (feature, labels, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### freq_encoding 扩展 cols 中的分类\n",
    "\n",
    "new_columns = dataframe[col].value_counts()[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function late in a list of features 'cols' from train and test dataset, \n",
    "# and performing frequency encoding. \n",
    "def freq_encoding(cols, train_df, test_df):\n",
    "    # we are going to store our new dataset in these two resulting datasets\n",
    "    result_train_df=pd.DataFrame()\n",
    "    result_test_df=pd.DataFrame()\n",
    "    \n",
    "    # loop through each feature column to do this\n",
    "    for col in cols:\n",
    "        \n",
    "        # capture the frequency of a feature in the training set in the form of a dataframe\n",
    "        col_freq=col+'_freq'\n",
    "        freq=train_df[col].value_counts()\n",
    "        freq=pd.DataFrame(freq)\n",
    "        freq.reset_index(inplace=True)\n",
    "        freq.columns=[[col,col_freq]]\n",
    "\n",
    "        # merge ths 'freq' datafarme with the train data\n",
    "        temp_train_df=pd.merge(train_df[[col]], freq, how='left', on=col)\n",
    "        temp_train_df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        # merge this 'freq' dataframe with the test data\n",
    "        temp_test_df=pd.merge(test_df[[col]], freq, how='left', on=col)\n",
    "        temp_test_df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        # if certain levels in the test dataset is not observed in the train dataset, \n",
    "        # we assign frequency of zero to them\n",
    "        temp_test_df.fillna(0, inplace=True)\n",
    "        temp_test_df[col_freq]=temp_test_df[col_freq].astype(np.int32)\n",
    "\n",
    "        if result_train_df.shape[0]==0:\n",
    "            result_train_df=temp_train_df\n",
    "            result_test_df=temp_test_df\n",
    "        else:\n",
    "            result_train_df=pd.concat([result_train_df, temp_train_df],axis=1)\n",
    "            result_test_df=pd.concat([result_test_df, temp_test_df],axis=1)\n",
    "    \n",
    "    return result_train_df, result_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ###  binary_encoding 扩展 feat 中的分类\n",
    "max_bin_len = len(\"{0:b}\".format(max_value))\n",
    "\n",
    "x = \"{0:b}\".format(35).zfill(max_bin_len)\n",
    "\n",
    "new_column = pd.Series(list(x)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform binary encoding for categorical variable\n",
    "# this function take in a pair of train and test data set, and the feature that need to be encode.\n",
    "# it returns the two dataset with input feature encoded in binary representation\n",
    "# this function assumpt that the feature to be encoded is already been encoded in a numeric manner \n",
    "# ranging from 0 to n-1 (n = number of levels in the feature). \n",
    "\n",
    "def binary_encoding(train_df, test_df, feat):\n",
    "    # calculate the highest numerical value used for numeric encoding\n",
    "    train_feat_max = train_df[feat].max()\n",
    "    test_feat_max = test_df[feat].max()\n",
    "    if train_feat_max > test_feat_max:\n",
    "        feat_max = train_feat_max\n",
    "    else:\n",
    "        feat_max = test_feat_max\n",
    "\n",
    "    # use the value of feat_max+1 to represent missing value\n",
    "    train_df.loc[train_df[feat] == 0, feat] = feat_max + 1\n",
    "    test_df.loc[test_df[feat] == 0, feat] = feat_max + 1\n",
    "    train_df.loc[train_df[feat] == -1, feat] = feat_max + 1\n",
    "    test_df.loc[test_df[feat] == -1, feat] = feat_max + 1\n",
    "\n",
    "    # create a union set of all possible values of the feature\n",
    "    union_val = np.union1d(train_df[feat].unique(), test_df[feat].unique())\n",
    "    # extract the highest value from from the feature in decimal format.\n",
    "    max_dec = union_val.max()\n",
    "\n",
    "    # work out how the ammount of digtis required to be represent max_dev in binary representation\n",
    "    max_bin_len = len(\"{0:b}\".format(max_dec))\n",
    "    index = np.arange(len(union_val))\n",
    "    columns = list([feat])\n",
    "    \n",
    "    # create a binary encoding feature dataframe to capture all the levels for the feature\n",
    "    bin_df = pd.DataFrame(index=index, columns=columns)\n",
    "    bin_df[feat] = union_val\n",
    "    \n",
    "    # capture the binary representation for each level of the feature \n",
    "    feat_bin = bin_df[feat].apply(lambda x: \"{0:b}\".format(x).zfill(max_bin_len))\n",
    "    \n",
    "    # split the binary representation into different bit of digits \n",
    "    splitted = feat_bin.apply(lambda x: pd.Series(list(x)).astype(np.uint8))\n",
    "    splitted.columns = [feat + '_bin_' + str(x) for x in splitted.columns]\n",
    "    bin_df = bin_df.join(splitted)\n",
    "    \n",
    "    # merge the binary feature encoding dataframe with the train and test dataset - Done! \n",
    "    train_df = pd.merge(train_df, bin_df, how='left', on=[feat])\n",
    "    test_df = pd.merge(test_df, bin_df, how='left', on=[feat])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cr = Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 17), (1, 38), (2, 38)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## 2. K-fold CV with Out-of-Fold Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 OOF utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "skf=StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Sklearn K-fold & OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_sklearn(clf, x_train, y_train, x_test, y_test, kf, scale=False, verbose=True, ruc=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    train_pred_prob = np.zeros((x_train.shape[0]))\n",
    "    dataset_blend_test_j = np.zeros((x_test.shape[0], n_folds))\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train[train_index], x_train[test_index]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        clf.fit(x_train_kf, y_train_kf)\n",
    "        train_pred[test_index] = clf.predict(x_val_kf)\n",
    "        train_pred_prob[test_index] = clf.predict(x_val_kf)\n",
    "        dataset_blend_test_j[:, i] = clf.predict(x_test)\n",
    "    test_pred = dataset_blend_test_j.sum(axis=1) // (n_folds //2 + 1)\n",
    "    return train_pred, test_pred, train_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Xgboost K-fold & OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability_to_rank(prediction, scaler=1):\n",
    "    pred_df=pd.DataFrame(columns=['probability'])\n",
    "    pred_df['probability']=prediction\n",
    "    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n",
    "    return pred_df['rank'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, y_test, kf, verbose=True,\n",
    "                      verbose_eval=50, num_boost_round=4000, use_rank=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    dataset_blend_test_j = np.zeros((x_test.shape[0], n_folds))\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train[train_index], x_train[test_index]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "        d_train_kf = xgb.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val_kf = xgb.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgb.DMatrix(x_test, y_test)\n",
    "        \n",
    "        bst = xgb.train(params, d_train_kf, num_boost_round=num_boost_round)\n",
    "        train_pred[test_index] = bst.predict(d_val_kf, ntree_limit=bst.best_ntree_limit)\n",
    "        dataset_blend_test_j[:, i] = bst.predict(d_test)\n",
    "        # test_pred+=probability_to_rank(bst.predict(d_test))\n",
    "    # test_pred /= kf.n_splits\n",
    "    test_pred = dataset_blend_test_j.sum(axis=1) // (n_folds //3 + 1)\n",
    "    return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 LigthGBM K-fold & OOF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, x_test, kf,\n",
    "                       verbose=True, verbose_eval=50, use_cat=True, use_rank=True):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    train_pred_prob = np.zeros((x_train.shape[0]))\n",
    "    dataset_blend_test_j = np.zeros((x_test.shape[0], n_folds))\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train[train_index], x_train[test_index]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "        lgb_train = lgb.Dataset(np.array(x_train_kf), y_train_kf)\n",
    "        lgb_val = lgb.Dataset(np.array(x_val_kf), y_val_kf, reference=lgb_train)\n",
    "        \n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=4000,\n",
    "                        valid_sets=lgb_val,\n",
    "                        early_stopping_rounds=30\n",
    "                    )\n",
    "        train_pred[test_index] = np.argmax(gbm.predict(x_val_kf), axis=1)\n",
    "        dataset_blend_test_j[:, i] =  np.argmax(gbm.predict(x_test), axis=1)\n",
    "    test_pred = dataset_blend_test_j.sum(axis=1) // (n_folds //2 + 1)\n",
    "    if verbose:\n",
    "        print('xgb train:{}'.format(classification_report(y_train, train_pred)))\n",
    "        print('xgb test:{}'.format(classification_report(y_test, test_pred)))\n",
    "    return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate level 1 OOF predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, n_jobs=6, max_depth=7,\n",
    "                           criterion='entropy', random_state=2018)\n",
    "rf_train_pred, rf_test_pred, _ = cross_validate_sklearn(rf, x_train=X_train, y_train=y_train,x_test=X_test, y_test=y_test, kf=skf,scale=False, verbose=True, ruc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100, criterion=\"gini\", max_depth=5,\n",
    "                          min_samples_split=5, n_jobs=-1, random_state=1)\n",
    "et_train_pred, et_test_pred, _ = cross_validate_sklearn(et, x_train=X_train, y_train=y_train,x_test=X_test, y_test=y_test, kf=skf,scale=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit=LogisticRegression(random_state=0, C=0.5,class_weight={0:1, 1:1})\n",
    "logit_train_pred, logit_test_pred, train_pred_prob = cross_validate_sklearn(logit, x_train=X_train, y_train=y_train,x_test=X_test, y_test=y_test, kf=skf,scale=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb=BernoulliNB()\n",
    "nb_train_pred, nb_test_pred, _ = cross_validate_sklearn(nb, x_train=X_train, y_train=y_train,x_test=X_test, y_test=y_test, kf=skf,scale=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" :  'multi:softmax',\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.70,\n",
    "    \"subsample\": 0.76,\n",
    "    \"colsample_bytree\": 0.95,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1,\n",
    "    'num_class': 3,\n",
    "}\n",
    "\n",
    "xgb_train_pred, xgb_test_pred = cross_validate_xgb(xgb_params, X_train, y_train, X_test,y_test, skf, use_rank=False, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\tvalid_0's auc: 0.5\n",
      "[3]\tvalid_0's auc: 0.5\n",
      "[4]\tvalid_0's auc: 0.5\n",
      "[5]\tvalid_0's auc: 0.5\n",
      "[6]\tvalid_0's auc: 0.5\n",
      "[7]\tvalid_0's auc: 0.5\n",
      "[8]\tvalid_0's auc: 0.5\n",
      "[9]\tvalid_0's auc: 0.5\n",
      "[10]\tvalid_0's auc: 0.5\n",
      "[11]\tvalid_0's auc: 0.5\n",
      "[12]\tvalid_0's auc: 0.5\n",
      "[13]\tvalid_0's auc: 0.5\n",
      "[14]\tvalid_0's auc: 0.5\n",
      "[15]\tvalid_0's auc: 0.5\n",
      "[16]\tvalid_0's auc: 0.5\n",
      "[17]\tvalid_0's auc: 0.5\n",
      "[18]\tvalid_0's auc: 0.5\n",
      "[19]\tvalid_0's auc: 0.5\n",
      "[20]\tvalid_0's auc: 0.5\n",
      "[21]\tvalid_0's auc: 0.5\n",
      "[22]\tvalid_0's auc: 0.5\n",
      "[23]\tvalid_0's auc: 0.5\n",
      "[24]\tvalid_0's auc: 0.5\n",
      "[25]\tvalid_0's auc: 0.5\n",
      "[26]\tvalid_0's auc: 0.5\n",
      "[27]\tvalid_0's auc: 0.5\n",
      "[28]\tvalid_0's auc: 0.5\n",
      "[29]\tvalid_0's auc: 0.5\n",
      "[30]\tvalid_0's auc: 0.5\n",
      "[31]\tvalid_0's auc: 0.5\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.5\n",
      "[1]\tvalid_0's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\tvalid_0's auc: 0.5\n",
      "[3]\tvalid_0's auc: 0.5\n",
      "[4]\tvalid_0's auc: 0.5\n",
      "[5]\tvalid_0's auc: 0.5\n",
      "[6]\tvalid_0's auc: 0.5\n",
      "[7]\tvalid_0's auc: 0.5\n",
      "[8]\tvalid_0's auc: 0.5\n",
      "[9]\tvalid_0's auc: 0.5\n",
      "[10]\tvalid_0's auc: 0.5\n",
      "[11]\tvalid_0's auc: 0.5\n",
      "[12]\tvalid_0's auc: 0.5\n",
      "[13]\tvalid_0's auc: 0.5\n",
      "[14]\tvalid_0's auc: 0.5\n",
      "[15]\tvalid_0's auc: 0.5\n",
      "[16]\tvalid_0's auc: 0.5\n",
      "[17]\tvalid_0's auc: 0.5\n",
      "[18]\tvalid_0's auc: 0.5\n",
      "[19]\tvalid_0's auc: 0.5\n",
      "[20]\tvalid_0's auc: 0.5\n",
      "[21]\tvalid_0's auc: 0.5\n",
      "[22]\tvalid_0's auc: 0.5\n",
      "[23]\tvalid_0's auc: 0.5\n",
      "[24]\tvalid_0's auc: 0.5\n",
      "[25]\tvalid_0's auc: 0.5\n",
      "[26]\tvalid_0's auc: 0.5\n",
      "[27]\tvalid_0's auc: 0.5\n",
      "[28]\tvalid_0's auc: 0.5\n",
      "[29]\tvalid_0's auc: 0.5\n",
      "[30]\tvalid_0's auc: 0.5\n",
      "[31]\tvalid_0's auc: 0.5\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.5\n",
      "[1]\tvalid_0's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\tvalid_0's auc: 0.5\n",
      "[3]\tvalid_0's auc: 0.5\n",
      "[4]\tvalid_0's auc: 0.5\n",
      "[5]\tvalid_0's auc: 0.5\n",
      "[6]\tvalid_0's auc: 0.5\n",
      "[7]\tvalid_0's auc: 0.5\n",
      "[8]\tvalid_0's auc: 0.5\n",
      "[9]\tvalid_0's auc: 0.5\n",
      "[10]\tvalid_0's auc: 0.5\n",
      "[11]\tvalid_0's auc: 0.5\n",
      "[12]\tvalid_0's auc: 0.5\n",
      "[13]\tvalid_0's auc: 0.5\n",
      "[14]\tvalid_0's auc: 0.5\n",
      "[15]\tvalid_0's auc: 0.5\n",
      "[16]\tvalid_0's auc: 0.5\n",
      "[17]\tvalid_0's auc: 0.5\n",
      "[18]\tvalid_0's auc: 0.5\n",
      "[19]\tvalid_0's auc: 0.5\n",
      "[20]\tvalid_0's auc: 0.5\n",
      "[21]\tvalid_0's auc: 0.5\n",
      "[22]\tvalid_0's auc: 0.5\n",
      "[23]\tvalid_0's auc: 0.5\n",
      "[24]\tvalid_0's auc: 0.5\n",
      "[25]\tvalid_0's auc: 0.5\n",
      "[26]\tvalid_0's auc: 0.5\n",
      "[27]\tvalid_0's auc: 0.5\n",
      "[28]\tvalid_0's auc: 0.5\n",
      "[29]\tvalid_0's auc: 0.5\n",
      "[30]\tvalid_0's auc: 0.5\n",
      "[31]\tvalid_0's auc: 0.5\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.5\n",
      "xgb train:             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      1.00      0.31        17\n",
      "          1       0.00      0.00      0.00        38\n",
      "          2       0.00      0.00      0.00        38\n",
      "\n",
      "avg / total       0.03      0.18      0.06        93\n",
      "\n",
      "xgb test:             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      1.00      0.40         8\n",
      "          1       0.00      0.00      0.00        12\n",
      "          2       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.06      0.25      0.10        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\develop\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'softmax',\n",
    "    'metric': {'auc'},\n",
    "    'num_leaves': 22,\n",
    "    'min_sum_hessian_in_leaf': 20,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_threads': 6,\n",
    "    'feature_fraction': 0.6894,\n",
    "    'bagging_fraction': 0.4218,\n",
    "    'max_drop': 5,\n",
    "    'drop_rate': 0.0023,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 0.01,\n",
    "    'verbose': 1,\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "\n",
    "cat_cols=[]\n",
    "lgm_train_pred, lgm_test_pred = cross_validate_lgb(lgb_params,X_train, y_train ,X_test, y_test, skf, use_cat=True, \n",
    "                            verbose_eval=False, use_rank=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Level 2 ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate L1 output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv1_train = np.stack((rf_train_pred, et_train_pred, logit_train_pred, nb_train_pred, xgb_train_pred, lgm_train_pred), axis=-1)\n",
    "lv1_test = np.stack((rf_test_pred, et_test_pred, logit_test_pred, nb_test_pred, xgb_test_pred, lgm_test_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Level 2 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" :  'multi:softmax',\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.70,\n",
    "    \"subsample\": 0.76,\n",
    "    \"colsample_bytree\": 0.95,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1,\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "lv2_xgb_train_pred, lv2_xgb_test_pred = cross_validate_xgb(xgb_params, lv1_train, y_train, lv1_test,y_test, skf, use_rank=False, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(lv1_train, y_train)  \n",
    "prediction = clf.predict(lv1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         8\n",
      "          1       0.92      0.92      0.92        12\n",
      "          2       0.92      0.92      0.92        12\n",
      "\n",
      "avg / total       0.94      0.94      0.94        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
